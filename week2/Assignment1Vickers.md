# Assignment 1: Git and GitHub Setup

## Overview
This assignment covers Git version control research, GitHub repository setup, and advanced exercises on AI ethics and technical limitations.

---

## Activities Completed

### Activity 1: Research Git
- **Documentation**: [Activity 1: Research Git](Week2Activiy1.md)
- **Coverage**: Comprehensive overview of Git architecture, version control systems, and core concepts

### Activity 2: Setting up a New GitHub and Local Repository
- **Repository**: [MSSE640-2025summer GitHub Repository](https://github.com/ThomasVickers-Regis/MSSE640-2025summer)
- **Setup**: Local repository configuration and GitHub integration

### Activity 3: Edit the README.md File and Push to GitHub Repository
- **Documentation**: [GitHub README](../README.md)
- **Implementation**: Updated project documentation and pushed changes to remote repository

---

## Advanced Exercises

### Background Reading
For those who completed this project in a previous class, please read one or more of the following articles. Your response will reference one or two of these articles based on the question you select:

#### Required Articles:
1. **[ChatGPT Is a Blurry JPEG of the Web](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web)** - Ted Chiang
2. **[Challenging the Myths of Generative AI](https://www.noemamag.com/challenging-the-myths-of-generative-ai/)** - Noema Magazine
3. **[On the Dangers of Stochastic Parrots](https://dl.acm.org/doi/10.1145/3442188.3445922)** - Bender et al.
4. **[Why AI Can't Spell 'Strawberry'](https://www.nytimes.com/2023/12/19/technology/ai-spelling-strawberry.html)** - The New York Times

---

### Question Selection
Choose **one** of the following questions that interests you most. Base your response on the referenced articles.

---

#### Question 1: Environmental and Social Responsibilities
**What responsibilities do software engineers have in mitigating the environmental and social impacts of large language models (LLMs)?**

**References**: "On the Dangers of Stochastic Parrots" and "Challenging the Myths of Generative AI"

**Discussion Points**:
- Discuss the trade-offs between model size, performance, and environmental cost
- Should engineers prioritize technical excellence over equity and sustainability?
- Consider the carbon footprint of training and deploying large AI models
- Examine the social implications of AI bias and misinformation

---

#### Question 2: AI Myths and Public Perception
**How do myths and metaphors about AI (e.g., "learning," "creativity," "productivity") influence public perception and policy?**

**Reference**: "Challenging the Myths of Generative AI"

**Discussion Points**:
- Explore how language shapes beliefs about AI's capabilities and limitations
- How can software engineers and educators challenge misleading narratives?
- Analyze the impact of AI terminology on public understanding
- Consider the role of media and marketing in AI perception

---

#### Question 3: Blurry JPEG Risks
**If ChatGPT is a "blurry JPEG of the Web," what are the risks of using such tools in academic, journalistic, or legal settings?**

**Reference**: "ChatGPT Is a Blurry JPEG of the Web"

**Discussion Points**:
- Evaluate the implications of lossy AI-generated content
- How do accuracy, originality, and accountability suffer or thrive in such systems?
- Consider the reliability of AI-generated information in critical contexts
- Examine the potential for misinformation and hallucination

---

#### Question 4: Technical Limitations of LLMs
**What does the inability of LLMs to spell "strawberry" reveal about the architecture of current AI systems?**

**Reference**: "Why AI Can't Spell 'Strawberry'"

**Discussion Points**:
- Analyze the technical limitations of transformer-based models with respect to fine-grained symbolic understanding
- How might future designs address this gap?
- Consider the relationship between training data and model capabilities
- Examine the fundamental differences between human and AI learning

---

#### Question 5: Model Size and Effectiveness
**Are larger AI models inherently better? When might smaller, curated models be preferable in software engineering practice?**

**References**: "On the Dangers of Stochastic Parrots" and "Challenging the Myths of Generative AI"

**Discussion Points**:
- Debate the scaling myth and consider real-world scenarios
- When might model size not correlate with usefulness or trustworthiness?
- Consider edge devices, local AI, and resource-constrained environments
- Examine the trade-offs between accessibility and performance

---

## Response Guidelines

### Format Requirements:
- **Length**: 500-800 words
- **Structure**: Clear introduction, body paragraphs, and conclusion
- **Citations**: Properly reference the selected articles
- **Style**: Professional, academic tone

### Content Expectations:
- Demonstrate understanding of the selected articles
- Provide thoughtful analysis of the chosen question
- Include relevant examples and evidence
- Consider multiple perspectives on the issue
- Connect to software engineering practices and responsibilities

### Submission:
- Include your response below this section
- Clearly indicate which question you selected
- Provide proper citations and references
- Submit through the designated course platform

---

## My Response

**Selected Question**: #3 - If ChatGPT is a "blurry JPEG of the Web," what are the risks of using such tools in academic, journalistic, or legal settings? Refer to: "ChatGPT Is a Blurry JPEG of the Web"
Evaluate the implications of lossy AI-generated content. How do accuracy, originality, and accountability suffer or thrive in such systems?

**Response**:
There is no question that AI is transformative and just getting started. The appeal of creating artificial intelligence that doesn't get tired and can perform any task a human can do is obvious. However, that is not the kind of AI we have today. LLMs, as shown in the "blurry JPEG" metaphor, highlight many issues with modern AI. AI is a vast subject area with many variations. Most breakthroughs today do not come directly from LLMs, but from combining traditional AI training on specific topics with LLMs to better understand the material they're trained on. These highly tailored AIs can make breakthroughs in fields by analyzing huge amounts of data to find accurate answers. LLMs alone can reason; however, their reasoning engines are mainly designed to keep the AI focused on the topic. You can't ask them to use all human knowledge to create a new cure for cancer. They can only return what was already discovered or created. That's why they seem to excel at learning things that take most people decades—because someone has already mastered it for them. A computer is great at processing large datasets quickly and providing answers, while a human takes years and still might be wrong. But those mistakes often drive scientific progress. People are passionate about their ideas and develop original concepts—even if they're wrong. Every famous scientist has been wrong about some ideas, and some were even considered radical for their time. Thanks to humans' ability to be original and explore strange ideas, some of the biggest breakthroughs in history have happened. AI doesn't have that same luxury because everyone expects its answers to be 100% factually correct. When a non-expert uses an LLM, they rely on the data it provides. If it designs an application in a specific way, there's likely a reason they didn't understand. It's only after someone becomes an expert and forms their own opinions on how to best do a task that they start noticing issues with LLMs. As AI improves in accuracy and reliability, new challenges will emerge for future generations—those who will only know an always-correct AI and depend on it. 


---

## References

1. Chiang, T. (2023). ChatGPT Is a Blurry JPEG of the Web. *The New Yorker*.
2. Noema Magazine. (2023). Challenging the Myths of Generative AI.
3. Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? *Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency*.
4. The New York Times. (2023). Why AI Can't Spell 'Strawberry'.
